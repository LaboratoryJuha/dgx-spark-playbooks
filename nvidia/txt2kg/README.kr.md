# 텍스트에서 지식 그래프로

> LLM 추론 및 그래프 시각화를 통해 비구조화된 텍스트를 대화형 지식 그래프로 변환

## 목차

- [개요](#개요)
- [지침](#지침)
- [문제 해결](#문제-해결)

---

## 개요

## 기본 개념

이 플레이북은 지식 그래프 추출을 위한 참조 역할을 하는 포괄적인 지식 그래프 생성 및 시각화 솔루션을 구축하고 배포하는 방법을 보여줍니다.
통합 메모리 아키텍처는 더 크고 정확한 모델을 실행할 수 있게 하여 더 높은 품질의 지식 그래프를 생성하고 우수한 다운스트림 GraphRAG 성능을 제공합니다.

이 txt2kg 플레이북은 다음을 사용하여 비구조화된 텍스트 문서를 구조화된 지식 그래프로 변환합니다:
- **지식 트리플 추출**: GPU 가속을 사용한 Ollama를 사용하여 로컬 LLM 추론으로 주어-술어-목적어 관계 추출
- **그래프 데이터베이스 저장**: 관계 순회와 함께 지식 트리플을 저장하고 쿼리하기 위한 ArangoDB
- **GPU 가속 시각화**: 대화형 2D/3D 그래프 탐색을 위한 Three.js WebGPU 렌더링

> **향후 개선 사항**: 벡터 임베딩 및 GraphRAG 기능은 계획된 개선 사항입니다.

## 달성할 목표

문서 처리, 지식 그래프 생성 및 편집, 쿼리를 제공하고 대화형 웹 인터페이스를 통해 액세스할 수 있는 완전한 기능의 시스템을 갖게 됩니다.
설정에는 다음이 포함됩니다:
- **로컬 LLM 추론**: API 키가 필요 없는 GPU 가속 LLM 추론을 위한 Ollama
- **그래프 데이터베이스**: 관계 순회와 함께 트리플을 저장하고 쿼리하기 위한 ArangoDB
- **대화형 시각화**: Three.js WebGPU를 사용한 GPU 가속 그래프 렌더링
- **현대적인 웹 인터페이스**: 문서 관리 및 쿼리 인터페이스가 있는 Next.js 프론트엔드
- **완전 컨테이너화**: Docker Compose 및 GPU 지원을 통한 재현 가능한 배포

## 필수 사항

-  최신 NVIDIA 드라이버가 설치된 DGX Spark
-  NVIDIA Container Toolkit으로 구성된 Docker 설치 및 구성
-  Docker Compose


## 시간 및 위험도

- **기간**:
  - 초기 설정 및 컨테이너 배포에 2-3분
  - Ollama 모델 다운로드에 5-10분 (모델 크기에 따라 다름)
  - 즉시 문서 처리 및 지식 그래프 생성

- **위험**:
  - GPU 메모리 요구 사항은 선택한 Ollama 모델 크기에 따라 다름
  - 문서 처리 시간은 문서 크기 및 복잡도에 따라 확장됨

- **롤백**: Docker 컨테이너를 중지하고 제거하며 필요한 경우 다운로드된 모델을 삭제
- **마지막 업데이트**: 12/02/2025
  - 다중 홉 그래프 순회를 사용한 지식 그래프 검색
  - 개선된 UI/UX

## 지침

## 1단계. 리포지토리 복제

터미널에서 txt2kg 리포지토리를 복제하고 프로젝트 디렉토리로 이동합니다.

```bash
git clone https://github.com/NVIDIA/dgx-spark-playbooks
cd dgx-spark-playbook/nvidia/txt2kg/assets
```

## 2단계. txt2kg 서비스 시작

제공된 시작 스크립트를 사용하여 모든 필수 서비스를 시작합니다. 이렇게 하면 Ollama, ArangoDB 및 Next.js 프론트엔드가 설정됩니다:

```bash
./start.sh
```

스크립트는 자동으로 다음을 수행합니다:
- GPU 가용성 확인
- Docker Compose 서비스 시작
- ArangoDB 데이터베이스 설정
- 웹 인터페이스 시작

## 3단계. Ollama 모델 가져오기 (선택 사항)

지식 추출을 위한 언어 모델을 다운로드합니다. 기본적으로 로드되는 모델은 Llama 3.1 8B입니다:

```bash
docker exec ollama-compose ollama pull <model-name>
```

사용 가능한 모델은 [https://ollama.com/search](https://ollama.com/search)에서 찾아보세요

> [!NOTE]
> 통합 메모리 아키텍처는 70B 매개변수와 같은 더 큰 모델을 실행할 수 있게 하여 훨씬 더 정확한 지식 트리플을 생성합니다.

## 4단계. 웹 인터페이스 액세스

브라우저를 열고 다음으로 이동합니다:

```
http://localhost:3001
```

개별 서비스에도 액세스할 수 있습니다:
- **ArangoDB 웹 인터페이스**: http://localhost:8529
- **Ollama API**: http://localhost:11434

## 5단계. 문서 업로드 및 지식 그래프 구축

#### 5.1. 문서 업로드
- 웹 인터페이스를 사용하여 텍스트 문서 업로드 (markdown, text, CSV 지원)
- 문서는 자동으로 청크화되고 트리플 추출을 위해 처리됨

#### 5.2. 지식 그래프 생성
- 시스템은 Ollama를 사용하여 주어-술어-목적어 트리플을 추출
- 트리플은 관계 쿼리를 위해 ArangoDB에 저장됨

#### 5.3. 대화형 시각화
- GPU 가속 렌더링을 사용하여 2D 또는 3D로 지식 그래프 보기
- 노드 및 관계를 대화형으로 탐색

#### 5.4. 그래프 기반 쿼리
- 쿼리 인터페이스를 사용하여 문서에 대한 질문하기
- 그래프 순회는 ArangoDB의 엔터티 관계로 컨텍스트를 향상시킴
- LLM은 강화된 그래프 컨텍스트를 사용하여 응답을 생성

> **향후 개선 사항**: 엔터티 검색을 위한 벡터 기반 KNN 검색을 사용한 GraphRAG 기능이 계획되어 있습니다.

## 6단계. 정리 및 롤백

모든 서비스를 중지하고 선택적으로 컨테이너를 제거합니다:

```bash
## 서비스 중지
docker compose down

## 컨테이너 및 볼륨 제거 (선택 사항)
docker compose down -v

## 다운로드된 모델 제거 (선택 사항)
docker exec ollama-compose ollama rm llama3.1:8b
```

## 7단계. 다음 단계

- 다양한 추출 품질을 위해 다양한 Ollama 모델 실험
- 도메인별 지식을 위한 트리플 추출 프롬프트 커스터마이징
- 고급 그래프 쿼리 및 시각화 기능 탐색

## 문제 해결

| 증상 | 원인 | 해결 방법 |
|---------|--------|-----|
| Ollama 성능 문제 | DGX Spark에 대한 최적이 아닌 설정 | 환경 변수 설정:<br>`OLLAMA_FLASH_ATTENTION=1` (더 나은 성능을 위해 플래시 어텐션 활성화)<br>`OLLAMA_KEEP_ALIVE=30m` (30분 동안 모델을 로드된 상태로 유지)<br>`OLLAMA_MAX_LOADED_MODELS=1` (VRAM 경합 방지)<br>`OLLAMA_KV_CACHE_TYPE=q8_0` (최소한의 성능 영향으로 KV 캐시 VRAM 감소) |
| VRAM 소진 또는 메모리 압력 (예: Ollama 모델 간 전환 시) | GPU 메모리를 소비하는 Linux 버퍼 캐시 | 버퍼 캐시 플러시: `sudo sync; sudo sh -c 'echo 3 > /proc/sys/vm/drop_caches'` |
| 느린 트리플 추출 | 큰 모델 또는 큰 컨텍스트 윈도우 | 문서 청크 크기를 줄이거나 더 빠른 모델 사용 |
| ArangoDB 연결 거부 | 서비스가 완전히 시작되지 않음 | start.sh 후 30초 대기, `docker ps`로 확인 |

> [!NOTE]
> DGX Spark는 GPU와 CPU 간의 동적 메모리 공유를 가능하게 하는 통합 메모리 아키텍처(UMA)를 사용합니다.
> 많은 애플리케이션이 여전히 UMA를 활용하도록 업데이트 중이므로 DGX Spark의 메모리 용량 내에 있어도
> 메모리 문제가 발생할 수 있습니다. 이 경우 다음을 사용하여 수동으로 버퍼 캐시를 플러시하세요:
```bash
sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
```
